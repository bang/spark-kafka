{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import expr\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('kafka-test'\n",
    "         ).config('spark.jars',\"jars/spark-sql-kafka-0-10_2.11-2.4.4.jar\" +\n",
    "                              \",jars/spark-streaming-kafka-assembly_2.11-1.6.3.jar\" \n",
    "         ).config('spark.driver.extraClassPath','jars/kafka-clients-2.4.0.jar'\n",
    "         ).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.show()\n",
    "    print(\"EPOCH_ID: {}\".format(epoch_id))\n",
    "    sleep(2)\n",
    "\n",
    "def process_row(row):\n",
    "    # Write row to storage\n",
    "    print(str(row['value']))\n",
    "    \n",
    "    \n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\"\n",
    "  ).option(\"kafka.bootstrap.servers\", \"localhost:9092\"\n",
    "  ).option(\"subscribe\", \"test\"\n",
    "  ).load(\n",
    "  ).select(col('key').cast('string')\n",
    "             ,col('value').cast('string')\n",
    "  ).writeStream.foreach(process_row).start() \n",
    "#.writeStream.foreach(foreach_batch_function).start()  \n",
    "\n",
    "#.writeStream \\\n",
    "#  .outputMode(\"append\"\n",
    "#  ).format(\"memory\"\n",
    "#  ).queryName('outTest'\n",
    "#  ).start()\n",
    "\n",
    "\n",
    "#.writeStream.foreachBatch(foreach_batch_function).start()  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from time import sleep\n",
    "#for x in range(10):\n",
    "#    spark.sql(\"SELECT * FROM outTest\").show()\n",
    "#    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
